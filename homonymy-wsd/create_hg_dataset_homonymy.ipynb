{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "",
   "id": "d184aebea3c09d3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:33:34.195695Z",
     "start_time": "2025-04-23T12:33:34.192441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from config import Credentials\n",
    "from ngram_utils import NgramFetcher"
   ],
   "id": "2d3ea59a6cd1295c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:33:36.028264Z",
     "start_time": "2025-04-23T12:33:35.917744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open('wn_homonyms.pkl', 'rb') as f:\n",
    "    wn_homonyms = pkl.load(f)"
   ],
   "id": "e71267f54cee0848",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:33:38.990819Z",
     "start_time": "2025-04-23T12:33:38.976057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "homonyms = {}\n",
    "for key, value in wn_homonyms.items():\n",
    "    if len(value) > 1:\n",
    "        homonyms[key] = value"
   ],
   "id": "db8c1832272416c1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:33:47.730916Z",
     "start_time": "2025-04-23T12:33:47.726758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filtered_homonyms = {}\n",
    "for key, value in homonyms.items():\n",
    "    word = key.split('.')[0]\n",
    "    word = word.replace('_', ' ')\n",
    "    word_type = key.split('.')[1]\n",
    "    # we do not want short words like 'a', 'b'\n",
    "    # we only take nouns for now, as, e.g. 'run' noun 'run' verb very similar meaning\n",
    "    if len(word) > 2 and word_type == 'n':\n",
    "        filtered_homonyms[word] = value\n",
    "\n",
    "word_data = [key for key in filtered_homonyms.keys()]"
   ],
   "id": "8923c14a2f060b98",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:55:42.879970Z",
     "start_time": "2025-04-23T12:55:42.873015Z"
    }
   },
   "cell_type": "code",
   "source": "pkl.dump(filtered_homonyms, open('filtered_homonyms.pkl', 'wb'))",
   "id": "e1e59f3898513ad4",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:40:37.926092Z",
     "start_time": "2025-04-23T12:40:37.422640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import semcor, wordnet as wn\n",
    "\n",
    "def count_occurence(target_synset: str):\n",
    "    target_synset = wn.synset(target_synset)\n",
    "    count = 0\n",
    "    for sent in semcor.tagged_sents(tag='sem'):\n",
    "        for chunk in sent:\n",
    "            if isinstance(chunk, nltk.tree.Tree):\n",
    "                if isinstance(chunk.label(), nltk.corpus.reader.wordnet.Lemma):\n",
    "                    synset = chunk.label().synset()\n",
    "                    if synset == target_synset:\n",
    "                        count += 1\n",
    "    return count"
   ],
   "id": "fbec22721a24de47",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:51:03.866557Z",
     "start_time": "2025-04-23T12:49:03.998165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "filtered_homonyms_w_occurences = {}\n",
    "for word, definitions in tqdm(filtered_homonyms.items()):\n",
    "    filtered_homonyms_w_occurences[word] = {}\n",
    "    for key, definition in definitions.items():\n",
    "        count = 0\n",
    "        for synset_tuple in definition:\n",
    "            synset = synset_tuple[0]\n",
    "            count += count_occurence(synset)\n",
    "        filtered_homonyms_w_occurences[word][key] = {'synsets': definition, 'semcor_occurances': count}"
   ],
   "id": "490eff3283ac3e97",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1791 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1791 [01:59<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m     12\u001B[39m     filtered_homonyms_w_occurences[word][key] = definition\n\u001B[32m     13\u001B[39m     \u001B[38;5;28mprint\u001B[39m(count)\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:1187\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython_darwin_312_64.SafeCallWrapper.__call__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:627\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython_darwin_312_64.PyDBFrame.trace_dispatch\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:937\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython_darwin_312_64.PyDBFrame.trace_dispatch\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:928\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython_darwin_312_64.PyDBFrame.trace_dispatch\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:585\u001B[39m, in \u001B[36m_pydevd_bundle.pydevd_cython_darwin_312_64.PyDBFrame.do_wait_suspend\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Applications/PyCharm Professional Edition.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1220\u001B[39m, in \u001B[36mPyDB.do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[39m\n\u001B[32m   1217\u001B[39m         from_this_thread.append(frame_id)\n\u001B[32m   1219\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, stop_reason):\n\u001B[32m-> \u001B[39m\u001B[32m1220\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Applications/PyCharm Professional Edition.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1235\u001B[39m, in \u001B[36mPyDB._do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[39m\n\u001B[32m   1232\u001B[39m             \u001B[38;5;28mself\u001B[39m._call_mpl_hook()\n\u001B[32m   1234\u001B[39m         \u001B[38;5;28mself\u001B[39m.process_internal_commands()\n\u001B[32m-> \u001B[39m\u001B[32m1235\u001B[39m         \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1237\u001B[39m \u001B[38;5;28mself\u001B[39m.cancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[32m   1239\u001B[39m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:53:40.174925Z",
     "start_time": "2025-04-23T12:53:39.954547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from itertools import islice\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Helper function to process one word\n",
    "def process_word(word_definitions_pair):\n",
    "    word, definitions = word_definitions_pair\n",
    "    word_result = {}\n",
    "\n",
    "    for key, definition in definitions.items():\n",
    "        total_count = 0\n",
    "        for synset_tuple in definition:\n",
    "            synset = synset_tuple[0]\n",
    "            if synset:\n",
    "                total_count += count_occurence(synset)\n",
    "        word_result[key] = {\n",
    "            'synsets': definition,\n",
    "            'semcor_occurances': total_count\n",
    "        }\n",
    "\n",
    "    return word, word_result\n",
    "\n",
    "# Limit to first 20 entries\n",
    "top_20_homonyms = list(islice(filtered_homonyms.items(), 20))\n",
    "\n",
    "filtered_homonyms_w_occurences = {}\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_word, item) for item in top_20_homonyms]\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Parallel processing\"):\n",
    "        word, result = future.result()\n",
    "        filtered_homonyms_w_occurences[word] = result"
   ],
   "id": "421ce23966317449",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parallel processing:   0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function process_word at 0x31c4a7420>: attribute lookup process_word on __main__ failed",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31m_RemoteTraceback\u001B[39m                          Traceback (most recent call last)",
      "\u001B[31m_RemoteTraceback\u001B[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py\", line 264, in _feed\n    obj = _ForkingPickler.dumps(obj)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/reduction.py\", line 51, in dumps\n    cls(buf, protocol).dump(obj)\n_pickle.PicklingError: Can't pickle <function process_word at 0x31c4a7420>: attribute lookup process_word on __main__ failed\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mPicklingError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 32\u001B[39m\n\u001B[32m     29\u001B[39m futures = [executor.submit(process_word, item) \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m top_20_homonyms]\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m future \u001B[38;5;129;01min\u001B[39;00m tqdm(as_completed(futures), total=\u001B[38;5;28mlen\u001B[39m(futures), desc=\u001B[33m\"\u001B[39m\u001B[33mParallel processing\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m     word, result = \u001B[43mfuture\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     33\u001B[39m     filtered_homonyms_w_occurences[word] = result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:449\u001B[39m, in \u001B[36mFuture.result\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    447\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[32m    448\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._state == FINISHED:\n\u001B[32m--> \u001B[39m\u001B[32m449\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    451\u001B[39m \u001B[38;5;28mself\u001B[39m._condition.wait(timeout)\n\u001B[32m    453\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401\u001B[39m, in \u001B[36mFuture.__get_result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    399\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception:\n\u001B[32m    400\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m401\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._exception\n\u001B[32m    402\u001B[39m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    403\u001B[39m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[32m    404\u001B[39m         \u001B[38;5;28mself\u001B[39m = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py:264\u001B[39m, in \u001B[36mQueue._feed\u001B[39m\u001B[34m(buffer, notempty, send_bytes, writelock, reader_close, writer_close, ignore_epipe, onerror, queue_sem)\u001B[39m\n\u001B[32m    261\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m    263\u001B[39m \u001B[38;5;66;03m# serialize the data before acquiring the lock\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m264\u001B[39m obj = \u001B[43m_ForkingPickler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    265\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m wacquire \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    266\u001B[39m     send_bytes(obj)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/reduction.py:51\u001B[39m, in \u001B[36mForkingPickler.dumps\u001B[39m\u001B[34m(cls, obj, protocol)\u001B[39m\n\u001B[32m     48\u001B[39m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[32m     49\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdumps\u001B[39m(\u001B[38;5;28mcls\u001B[39m, obj, protocol=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m     50\u001B[39m     buf = io.BytesIO()\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m     \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbuf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     52\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m buf.getbuffer()\n",
      "\u001B[31mPicklingError\u001B[39m: Can't pickle <function process_word at 0x31c4a7420>: attribute lookup process_word on __main__ failed"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T07:23:23.830008Z",
     "start_time": "2025-04-23T07:23:20.577348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\n",
    "    \"lukasellinger/homonym-homonymy-wsd\",\n",
    "    token=Credentials.hf_api_key\n",
    ")['train']"
   ],
   "id": "a3d9e6082c672bbe",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T07:23:23.845542Z",
     "start_time": "2025-04-23T07:23:23.832932Z"
    }
   },
   "cell_type": "code",
   "source": "word_data = [entry['word'] for entry in dataset]",
   "id": "5052d84fc9dd5088",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T07:28:19.106252Z",
     "start_time": "2025-04-23T07:23:25.136341Z"
    }
   },
   "cell_type": "code",
   "source": "ngram_data = NgramFetcher().fetch_ngram_data(word_data, inflections=True)",
   "id": "53b8b0aca07c036",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Ngram data for koweit\n",
      "No Ngram data for saint john's\n",
      "No Ngram data for santiago de cuba\n",
      "No Ngram data for william cowper\n",
      "No Ngram data for johann strauss\n",
      "No Ngram data for river avon\n",
      "No Ngram data for somme river\n",
      "No Ngram data for cynoscephalae\n",
      "No Ngram data for william gilbert\n",
      "No Ngram data for severn river\n",
      "No Ngram data for john trumbull\n",
      "No Ngram data for ferdinand i\n",
      "No Ngram data for battle of ypres\n",
      "No Ngram data for coeur d'alene\n",
      "No Ngram data for bismarck sea\n",
      "No Ngram data for frederick i\n",
      "No Ngram data for odessa\n",
      "No Ngram data for siege of syracuse\n",
      "No Ngram data for arthur schlesinger\n",
      "No Ngram data for marston moor\n",
      "No Ngram data for president harrison\n",
      "No Ngram data for hohenlinden\n",
      "No Ngram data for arab-israeli war\n",
      "No Ngram data for battle of the somme\n",
      "No Ngram data for cape passero\n",
      "No Ngram data for thomas wolfe\n",
      "No Ngram data for capital of georgia\n",
      "No Ngram data for william seward burroughs\n",
      "No Ngram data for naseby\n",
      "No Ngram data for thomas hart benton\n",
      "No Ngram data for meuse river\n",
      "No Ngram data for samuel butler\n",
      "No Ngram data for president adams\n",
      "No Ngram data for joliot-curie\n",
      "No Ngram data for el alamein\n",
      "No Ngram data for anapurna\n",
      "No Ngram data for saint thomas\n",
      "No Ngram data for guarneri\n",
      "No Ngram data for guarnerius\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T06:04:26.222389Z",
     "start_time": "2025-04-23T06:04:26.219690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "missing = []\n",
    "for key, value in ngram_data.items():\n",
    "    if 'avg_frequency' not in value:\n",
    "        missing.append(key)"
   ],
   "id": "528fa5b72102a4d0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T06:03:27.538527Z",
     "start_time": "2025-04-23T06:03:21.363934Z"
    }
   },
   "cell_type": "code",
   "source": "ngram_data_missing = NgramFetcher().fetch_ngram_data(missing)",
   "id": "59178bbbf3a5535f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid Ngram data returned for batch: ['koweit', \"saint john's\", 'santiago de cuba', 'william cowper', 'johann strauss', 'river avon', 'somme river', 'cynoscephalae', 'william gilbert', 'severn river']\n",
      "No valid Ngram data returned for batch: ['john trumbull', 'ferdinand i', 'battle of ypres', \"coeur d'alene\", 'bismarck sea', 'frederick i', 'siege of syracuse', 'arthur schlesinger', 'marston moor', 'president harrison']\n",
      "No valid Ngram data returned for batch: ['hohenlinden', 'arab-israeli war', 'battle of the somme', 'cape passero', 'thomas wolfe', 'capital of georgia', 'william seward burroughs', 'naseby', 'thomas hart benton', 'meuse river']\n",
      "No valid Ngram data returned for batch: ['samuel butler', 'president adams', 'joliot-curie', 'el alamein', 'anapurna', 'guarneri', 'guarnerius']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T06:08:30.720072Z",
     "start_time": "2025-04-23T06:08:30.717587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = []\n",
    "for word in word_data:\n",
    "    avg_frequency = avg_freq = ngram_data.get(word, {}).get(\"avg_frequency\") or ngram_data_missing.get(word, {}).get(\"avg_frequency\")\n",
    "    data.append({'word': word, 'avg_google_ngrams_frequency':avg_frequency})"
   ],
   "id": "d03324fcd659eb4a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T08:47:11.009797Z",
     "start_time": "2025-04-21T08:47:11.007123Z"
    }
   },
   "cell_type": "code",
   "source": "pkl.dump(data, open('homonym-homonymy-wsd.pkl', 'wb'))",
   "id": "300539e91e00f2fe",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T06:33:17.680473Z",
     "start_time": "2025-04-29T06:33:13.269080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\n",
    "    \"lukasellinger/homonym-homonymy-wsd\",\n",
    "    token=Credentials.hf_api_key\n",
    ")['train']"
   ],
   "id": "56382f53400ac453",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T06:35:27.259058Z",
     "start_time": "2025-04-29T06:35:27.249544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('../filtered_homonyms_w_occurences.pkl', 'rb') as f:\n",
    "    filtered_homonyms_w_occurences = pkl.load(f)"
   ],
   "id": "8874e4008e6672b9",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T06:59:36.826531Z",
     "start_time": "2025-04-29T06:59:21.386691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = []\n",
    "for word, v in filtered_homonyms_w_occurences.items():\n",
    "    for entry in dataset:\n",
    "        if entry['word'] == word:\n",
    "            avg_google_ngrams_frequency = entry['avg_google_ngrams_frequency']\n",
    "            coarse_synsets = [{\"name\": name,\n",
    "                               \"semcor_occurances\": info['semcor_occurances'],\n",
    "                               \"synsets\": [{\"name\": synset[0], \"definition\": synset[1]} for synset in info['synsets']]} for name, info in v.items()]\n",
    "            data.append({'word': word, 'avg_google_ngrams_frequency': avg_google_ngrams_frequency, 'coarse_synsets': coarse_synsets})\n",
    "            break"
   ],
   "id": "c08782f6feaa2f10",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T07:00:06.383847Z",
     "start_time": "2025-04-29T07:00:06.366984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_list(data)"
   ],
   "id": "1172e0570ba34f4d",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T07:00:16.975650Z",
     "start_time": "2025-04-29T07:00:13.585887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset.push_to_hub(\n",
    "    repo_id=\"lukasellinger/homonym-homonymy-wsd\",\n",
    "    private=True,\n",
    "    token=Credentials.hf_api_key\n",
    ")"
   ],
   "id": "5c5f1abcf4bfecc3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 396.44ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/lukasellinger/homonym-homonymy-wsd/commit/844bc29807e56e9e9a38d5d106992e77ac8a8ef5', commit_message='Upload dataset', commit_description='', oid='844bc29807e56e9e9a38d5d106992e77ac8a8ef5', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/lukasellinger/homonym-homonymy-wsd', endpoint='https://huggingface.co', repo_type='dataset', repo_id='lukasellinger/homonym-homonymy-wsd'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
